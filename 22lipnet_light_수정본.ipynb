{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "22lipnet_light_수정본.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tae-min-lee/t-m-po/blob/master/22lipnet_light_%EC%88%98%EC%A0%95%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKkRalaAN0pl"
      },
      "source": [
        "from tensorflow.keras.layers import Lambda\n",
        "\n",
        "from tensorflow.keras.layers import BatchNormalization, GRU, Conv3D, ZeroPadding3D,MaxPooling3D,Dense, Activation, SpatialDropout3D, Flatten, Input, Bidirectional, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import copy, cv2, os, random, glob, shutil\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ7c_98UN0ps"
      },
      "source": [
        "# Prepare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf05b-B_N0pt",
        "outputId": "4bde897c-b412-4b0d-cea8-35d2252b0c46"
      },
      "source": [
        " import copy, cv2, os, random, glob, shutil\n",
        " import numpy as np\n",
        "\n",
        " video_root = r'./train_dataset/video' # video파일이 있는 주소\n",
        " align_root =  r'./train_dataset/align' # align파일이 있는 주소\n",
        " video_dirlist = glob.glob(video_root + '/*') # video 디렉토리를 읽어온다\n",
        "\n",
        " train_align_path = []\n",
        " train_video_path = []\n",
        "\n",
        " for k in range(len(video_dirlist)):\n",
        "     print(video_dirlist[k])\n",
        "     video_path_ACEN = []\n",
        "    video_ids = os.listdir(video_dirlist[k])# 각 speaker들의 디렉토리를 가져온다\n",
        "    # print(video_ids)\n",
        "    \n",
        "     for video_id in video_ids:\n",
        "         video_path_ACEN = glob.glob(video_dirlist[k]+'/'+video_id + '/*.png')# 각speaker들이 말한 이미지 파일을 읽어온다\n",
        "         train_video_path.append(video_path_ACEN)# train_video에 추가를 한ㄴ다\n",
        "         train_align_path.append(align_root+'/'+video_id+'.align')# align파일을 추가한다"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./train_dataset/video\\s10\n",
            "./train_dataset/video\\s11\n",
            "./train_dataset/video\\s12\n",
            "./train_dataset/video\\s13\n",
            "./train_dataset/video\\s14\n",
            "./train_dataset/video\\s15\n",
            "./train_dataset/video\\s16\n",
            "./train_dataset/video\\s17\n",
            "./train_dataset/video\\s18\n",
            "./train_dataset/video\\s19\n",
            "./train_dataset/video\\s23\n",
            "./train_dataset/video\\s24\n",
            "./train_dataset/video\\s25\n",
            "./train_dataset/video\\s26\n",
            "./train_dataset/video\\s27\n",
            "./train_dataset/video\\s28\n",
            "./train_dataset/video\\s29\n",
            "./train_dataset/video\\s3\n",
            "./train_dataset/video\\s30\n",
            "./train_dataset/video\\s31\n",
            "./train_dataset/video\\s32\n",
            "./train_dataset/video\\s33\n",
            "./train_dataset/video\\s34\n",
            "./train_dataset/video\\s4\n",
            "./train_dataset/video\\s5\n",
            "./train_dataset/video\\s6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEntrsCON0px",
        "outputId": "46ab636f-c865-425f-c521-f06c87f2a73c"
      },
      "source": [
        "\n",
        " video_root = r'./val_dataset/video'\n",
        " align_root =  r'./train_dataset/align'\n",
        " video_dirlist = glob.glob(video_root + '/*')\n",
        "\n",
        " val_align_path = []\n",
        " val_video_path = []\n",
        "\n",
        " for k in range(len(video_dirlist)):\n",
        "     print(video_dirlist[k])\n",
        "     video_path_ACEN = []\n",
        "          video_ids = os.listdir(video_dirlist[k])\n",
        "    # print(video_ids)\n",
        "    \n",
        "     for video_id in video_ids:\n",
        "         video_path_ACEN = glob.glob(video_dirlist[k]+'/'+video_id + '/*.png')\n",
        "         val_video_path.append(video_path_ACEN)\n",
        "         val_align_path.append(align_root+'/'+video_id+'.align')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./val_dataset/video\\s1\n",
            "./val_dataset/video\\s2\n",
            "./val_dataset/video\\s20\n",
            "./val_dataset/video\\s22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5nnuSlcN0pz"
      },
      "source": [
        "## save pickle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Sv-XeY3N0p0"
      },
      "source": [
        "# import pickle\n",
        "# name = './train_video_path.pickle'\n",
        "# with open(name,'wb') as fw:\n",
        "#     pickle.dump(train_video_path, fw)\n",
        "    \n",
        "# name = './train_align_path.pickle'\n",
        "# with open(name,'wb') as fw:\n",
        "#     pickle.dump(train_align_path, fw)\n",
        "    \n",
        "# name = './val_video_path.pickle'\n",
        "# with open(name,'wb') as fw:\n",
        "#     pickle.dump(val_video_path, fw)\n",
        "    \n",
        "# name = './val_align_path.pickle'\n",
        "# with open(name,'wb') as fw:\n",
        "#     pickle.dump(val_align_path, fw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTdXvG9vN0p2"
      },
      "source": [
        "## load pickle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC4Sa-TQN0p3"
      },
      "source": [
        "import pickle # 파이썬 객체 자체를 파일로 저장하는 것\n",
        "name = './train_video_path.pickle'\n",
        "with open(name,'rb') as fw:\n",
        "    train_video_path = pickle.load(fw)\n",
        "    \n",
        "name = './train_align_path.pickle'\n",
        "with open(name,'rb') as fw:\n",
        "    train_align_path = pickle.load(fw)\n",
        "    \n",
        "name = './val_video_path.pickle'\n",
        "with open(name,'rb') as fw:\n",
        "    val_video_path = pickle.load(fw)\n",
        "    \n",
        "name = './val_align_path.pickle'\n",
        "with open(name,'rb') as fw:\n",
        "    val_align_path = pickle.load(fw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwA3oLgeN0p5",
        "outputId": "ee9b0d0f-8239-4ca8-95f1-dc5c9a522f0d"
      },
      "source": [
        "print(train_align_path[8082][22:-6])\n",
        "print(train_video_path[8081-1][0][:-20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bgir9p\n",
            "./train_dataset/video\\s18/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSca0VxWN0p8"
      },
      "source": [
        "## 폴더속 이미지의 크기가 75가 아닌경우는 모두 삭제"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnYo2sSVN0p8"
      },
      "source": [
        "# for check in range(len(train_video_path)):\n",
        "#     if len(train_video_path[check])!=75:\n",
        "#         print(train_align_path[check])\n",
        "#         print(train_video_path[check-1][0])\n",
        "#         shutil.rmtree(train_video_path[check-1][0][:-20]+train_align_path[check][22:-6])\n",
        "#         print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51qCaIyIN0p_",
        "outputId": "e34d318a-8f71-47c1-a0eb-51f9e4614969"
      },
      "source": [
        "len(val_align_path),len(val_video_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3964, 3964)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt4HZW9ON0qC"
      },
      "source": [
        "## Datagenerator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iob3gQ2UN0qC"
      },
      "source": [
        "def text_to_labels(text): # text를 label로 변환하는 함수\n",
        "    ret = []\n",
        "    for char in text:\n",
        "        if char >= 'a' and char <= 'z': # 만약 텍스트가 a에서 z사이인 문자라면 \n",
        "            ret.append(ord(char) - ord('a'))# 그 택스트를 아스키코드 값으로 바꾸고 a의 아스키코드 값을 빼준다\n",
        "        elif char == ' ':# 만약 공백이라면\n",
        "            ret.append(26) #26을 추가해준다\n",
        "    return ret\n",
        "\n",
        "def labels_to_text(labels):# label을 text로 바꿔주는 함수\n",
        "    # 26 is space, 27 is CTC blank char\n",
        "    text = ''\n",
        "    for c in labels:\n",
        "        if c >= 0 and c < 26:\n",
        "            text += chr(c + ord('a'))# 아스키코드값을 사용하여 label값으로 (문자)로 바꿔준다\n",
        "        elif c == 26: #26이라면 blank로 바꿔준다\n",
        "            text += ' '\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qui7SpV3N0qE"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Align(object):\n",
        "    def __init__(self, absolute_max_string_len=32, label_func=None):\n",
        "        self.label_func = label_func\n",
        "        self.absolute_max_string_len = absolute_max_string_len\n",
        "\n",
        "    def from_file(self, path):\n",
        "        with open(path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        align = [(int(y[0])/1000, int(y[1])/1000, y[2]) for y in [x.strip().split(\" \") for x in lines]]\n",
        "        self.build(align)\n",
        "        return self\n",
        "\n",
        "    def from_array(self, align):\n",
        "        self.build(align)\n",
        "        return self\n",
        "\n",
        "    def build(self, align):\n",
        "        self.align = self.strip(align, ['sp','sil'])\n",
        "        self.sentence = self.get_sentence(align)\n",
        "        self.label = self.get_label(self.sentence)\n",
        "        self.padded_label = self.get_padded_label(self.label)\n",
        "\n",
        "    def strip(self, align, items):\n",
        "        return [sub for sub in align if sub[2] not in items]\n",
        "\n",
        "    def get_sentence(self, align):\n",
        "        return \" \".join([y[-1] for y in align if y[-1] not in ['sp', 'sil']])\n",
        "\n",
        "    def get_label(self, sentence):\n",
        "        return self.label_func(sentence)\n",
        "\n",
        "    def get_padded_label(self, label):\n",
        "        padding = np.ones((self.absolute_max_string_len-len(label))) * -1\n",
        "        return np.concatenate((np.array(label), padding), axis=0)\n",
        "\n",
        "    @property\n",
        "    def word_length(self):\n",
        "        return len(self.sentence.split(\" \"))\n",
        "\n",
        "    @property\n",
        "    def sentence_length(self):\n",
        "        return len(self.sentence)\n",
        "\n",
        "    @property\n",
        "    def label_length(self):\n",
        "        return len(self.label)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWc4EBjJN0qG"
      },
      "source": [
        "def data_generator(video,align,batch_size = 32) :\n",
        "\n",
        " \n",
        "    \n",
        "    video_path = video # 각각 video와 align을 받는다\n",
        "    align_path = align\n",
        "    #random.shuffle(idx_list)\n",
        "\n",
        "    while 1: # 무한루프\n",
        "        idx = 0\n",
        "        idx_list = list(range(len(video_path))) # 비디오의 갯수를 길이로 숫자로 list를 만든다\n",
        "\n",
        "        batch_imgs = []\n",
        "        batch_labs = []\n",
        "        label_length = []\n",
        "        input_length = []\n",
        "        source_str = []\n",
        "\n",
        "        if idx > len(idx_list) - batch_size:# batch size를 뺸 값이 idx보다 클 때\n",
        "            tmp_list =idx_list[idx:] #idx_list의 idx부터 끝까지를 tmp_list로 넣는다\n",
        "            idx = 0\n",
        "        else: #아니라면\n",
        "            tmp_list = idx_list[idx:idx+batch_size]# idx부터 batch size를 더한만큼을 tmp_list에 넣는다\n",
        "            idx = idx + batch_size\n",
        "\n",
        "        for i in tmp_list:\n",
        "\n",
        "            imgs=[]\n",
        "            for v in video_path[i]:# batch_size만큼부터 차례로 cv2를 이용하여 이미지를 읽어온다\n",
        "                img = cv2.imread(v)/255\n",
        "                imgs.append(img)# 읽어온 이미지를 imgs에 추가한다\n",
        "\n",
        "            align = Align(32, text_to_labels).from_file(align_path[i]) # align도 배치 크기 많큼 가져온다\n",
        "\n",
        "            batch_labs.append(align.padded_label)\n",
        "            batch_imgs.append(np.array(imgs))\n",
        "            #source_str.append(align.sentence)\n",
        "            label_length.append(align.label_length)\n",
        "            input_length.append(len(imgs))\n",
        "\n",
        "        #source_str = np.array(source_str)\n",
        "        label_length = np.array(label_length) #align에 대한것\n",
        "        input_length = np.array(input_length)# img에 대한것\n",
        "        batch_labs = np.array(batch_labs)\n",
        "        batch_imgs = np.array(batch_imgs)\n",
        "\n",
        "        inputs = [batch_imgs, batch_labs, input_length, label_length]\n",
        "\n",
        "        outputs =  np.zeros([batch_size])  # dummy data for dummy loss function\n",
        "\n",
        "        yield inputs, outputs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKDTgIA0N0qI"
      },
      "source": [
        "# #random.shuffle(idx_list)\n",
        "# video_path = train_video_path\n",
        "# align_path = train_align_path\n",
        "\n",
        "# dddddd=1\n",
        "# while 1:\n",
        "#     idx = 0\n",
        "#     batch_imgs = []\n",
        "#     batch_labs = []\n",
        "#     batch_size = 64\n",
        "#     label_length = []\n",
        "#     input_length = []\n",
        "#     source_str = []\n",
        "\n",
        "\n",
        "\n",
        "#     idx_list = list(range(len(video_path)))\n",
        "#     #random.shuffle(idx_list)\n",
        "\n",
        "#     batch_imgs = []\n",
        "#     batch_labs = []\n",
        "#     label_length = []\n",
        "#     input_length = []\n",
        "#     source_str = []\n",
        "\n",
        "#     if idx > len(idx_list) - batch_size:\n",
        "#         tmp_list =idx_list[idx:]\n",
        "#         idx = 0\n",
        "#     else:\n",
        "#         tmp_list = idx_list[idx:idx+batch_size]\n",
        "#         idx = idx + batch_size\n",
        "\n",
        "#     for i in tmp_list:\n",
        "\n",
        "#         imgs=[]\n",
        "#         for v in video_path[i]:\n",
        "#             img = cv2.imread(v)\n",
        "#             imgs.append(img)\n",
        "\n",
        "#         align = Align(32, text_to_labels).from_file(align_path[i])\n",
        "\n",
        "#         batch_labs.append(align.padded_label)\n",
        "#         batch_imgs.append(imgs)\n",
        "#         source_str.append(align.sentence)\n",
        "#         label_length.append(align.label_length)\n",
        "#         input_length.append(len(imgs))\n",
        "\n",
        "#     source_str = np.array(source_str)\n",
        "#     label_length = np.array(label_length)\n",
        "#     input_length = np.array(input_length)\n",
        "#     batch_labs = np.array(batch_labs)\n",
        "#     batch_imgs = np.array(batch_imgs)\n",
        "\n",
        "#     inputs = {'the_input': batch_imgs,\n",
        "#               'the_labels': batch_labs,\n",
        "#               'input_length': input_length,\n",
        "#               'label_length': label_length,\n",
        "#               'source_str': source_str\n",
        "#               }\n",
        "\n",
        "#     outputs =  np.zeros([batch_size]) \n",
        "\n",
        "#     print(dddddd)\n",
        "#     print(inputs['the_input'].shape)\n",
        "#     print(inputs['the_labels'].shape)\n",
        "#     print(inputs['input_length'].shape)\n",
        "#     print(inputs['label_length'].shape)\n",
        "#     print(outputs.shape)\n",
        "#     dddddd=dddddd+1\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmtKn3FjN0qM"
      },
      "source": [
        "# CTC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv9KyITON0qM"
      },
      "source": [
        "def CTC(name, args):\n",
        "\treturn Lambda(ctc_lambda_func, output_shape=(1,), name=name)(args)\n",
        "\n",
        "def ctc_lambda_func(args):\n",
        "    y_pred, labels, input_length, label_length = args\n",
        "    # From Keras example image_ocr.py:\n",
        "    # the 2 is critical here since the first couple outputs of the RNN\n",
        "    # tend to be garbage:\n",
        "    # y_pred = y_pred[:, 2:, :]\n",
        "    y_pred = y_pred[:, :, :]\n",
        "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0HQDLK_N0qP"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJXBnUKbN0qP"
      },
      "source": [
        "class LipNet(object):\n",
        "    def __init__(self, img_c=3, img_w=100, img_h=50, frames_n=75, absolute_max_string_len=32, output_size=28):\n",
        "        self.img_c = img_c\n",
        "        self.img_w = img_w\n",
        "        self.img_h = img_h\n",
        "        self.frames_n = frames_n\n",
        "        self.absolute_max_string_len = absolute_max_string_len\n",
        "        self.output_size = output_size\n",
        "        self.build()\n",
        "\n",
        "    def build(self):\n",
        "        input_shape = (self.frames_n, self.img_h, self.img_w, self.img_c)\n",
        "\n",
        "        self.input_data = Input(name='the_input', shape=input_shape, dtype='float32')\n",
        "\n",
        "        self.zero1 = ZeroPadding3D(padding=(1, 2, 2), name='zero1')(self.input_data)\n",
        "        self.conv1 = Conv3D(32, (3, 5, 5), strides=(1, 2, 2), kernel_initializer='he_normal', name='conv1')(self.zero1)\n",
        "        self.batc1 = BatchNormalization(name='batc1')(self.conv1)\n",
        "        self.actv1 = Activation('relu', name='actv1')(self.batc1)\n",
        "        self.drop1 = SpatialDropout3D(0.5)(self.actv1)\n",
        "        self.maxp1 = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max1')(self.drop1)\n",
        "\n",
        "        self.zero2 = ZeroPadding3D(padding=(1, 2, 2), name='zero2')(self.maxp1)\n",
        "        self.conv2 = Conv3D(64, (3, 5, 5), strides=(1, 1, 1), kernel_initializer='he_normal', name='conv2')(self.zero2)\n",
        "        self.batc2 = BatchNormalization(name='batc2')(self.conv2)\n",
        "        self.actv2 = Activation('relu', name='actv2')(self.batc2)\n",
        "        self.drop2 = SpatialDropout3D(0.5)(self.actv2)\n",
        "        self.maxp2 = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max2')(self.drop2)\n",
        "\n",
        "        self.zero3 = ZeroPadding3D(padding=(1, 1, 1), name='zero3')(self.maxp2)\n",
        "        self.conv3 = Conv3D(96, (3, 3, 3), strides=(1, 1, 1), kernel_initializer='he_normal', name='conv3')(self.zero3)\n",
        "        self.batc3 = BatchNormalization(name='batc3')(self.conv3)\n",
        "        self.actv3 = Activation('relu', name='actv3')(self.batc3)\n",
        "        self.drop3 = SpatialDropout3D(0.5)(self.actv3)\n",
        "        self.maxp3 = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max3')(self.drop3)\n",
        "\n",
        "        self.resh1 = TimeDistributed(Flatten())(self.maxp3)\n",
        "\n",
        "        self.gru_1 = Bidirectional(GRU(256, return_sequences=True, kernel_initializer='Orthogonal', name='gru1'), merge_mode='concat')(self.resh1)\n",
        "        self.gru_2 = Bidirectional(GRU(256, return_sequences=True, kernel_initializer='Orthogonal', name='gru2'), merge_mode='concat')(self.gru_1)\n",
        "\n",
        "        # transforms RNN output to character activations:\n",
        "        self.dense1 = Dense(self.output_size, kernel_initializer='he_normal', name='dense1')(self.gru_2)\n",
        "\n",
        "        self.y_pred = Activation('softmax', name='softmax')(self.dense1)\n",
        "\n",
        "        self.labels = Input(name='the_labels', shape=[self.absolute_max_string_len], dtype='float32')\n",
        "        self.input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
        "        self.label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
        "\n",
        "        self.loss_out = CTC('ctc', [self.y_pred, self.labels, self.input_length, self.label_length])\n",
        "\n",
        "        self.model = Model(inputs=[self.input_data, self.labels, self.input_length, self.label_length], outputs=self.loss_out)\n",
        "\n",
        "    def summary(self):\n",
        "        Model(inputs=self.input_data, outputs=self.y_pred).summary()\n",
        "\n",
        "    def predict(self, input_batch):\n",
        "        return self.test_function([input_batch, 0])[0]  # the first 0 indicates test\n",
        "\n",
        "    @property\n",
        "    def test_function(self):\n",
        "        # captures output of softmax so we can decode the output during visualization\n",
        "        return K.function([self.input_data, K.learning_phase()], [self.y_pred, K.learning_phase()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHZJjz0SN0qS"
      },
      "source": [
        "lipnet = LipNet()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "OUbzH2JCN0qV",
        "outputId": "0c919dda-d718-4611-960d-4566225e13cd"
      },
      "source": [
        "lipnet.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "the_input (InputLayer)       [(None, 75, 50, 100, 3)]  0         \n",
            "_________________________________________________________________\n",
            "zero1 (ZeroPadding3D)        (None, 77, 54, 104, 3)    0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv3D)               (None, 75, 25, 50, 32)    7232      \n",
            "_________________________________________________________________\n",
            "batc1 (BatchNormalization)   (None, 75, 25, 50, 32)    128       \n",
            "_________________________________________________________________\n",
            "actv1 (Activation)           (None, 75, 25, 50, 32)    0         \n",
            "_________________________________________________________________\n",
            "spatial_dropout3d (SpatialDr (None, 75, 25, 50, 32)    0         \n",
            "_________________________________________________________________\n",
            "max1 (MaxPooling3D)          (None, 75, 12, 25, 32)    0         \n",
            "_________________________________________________________________\n",
            "zero2 (ZeroPadding3D)        (None, 77, 16, 29, 32)    0         \n",
            "_________________________________________________________________\n",
            "conv2 (Conv3D)               (None, 75, 12, 25, 64)    153664    \n",
            "_________________________________________________________________\n",
            "batc2 (BatchNormalization)   (None, 75, 12, 25, 64)    256       \n",
            "_________________________________________________________________\n",
            "actv2 (Activation)           (None, 75, 12, 25, 64)    0         \n",
            "_________________________________________________________________\n",
            "spatial_dropout3d_1 (Spatial (None, 75, 12, 25, 64)    0         \n",
            "_________________________________________________________________\n",
            "max2 (MaxPooling3D)          (None, 75, 6, 12, 64)     0         \n",
            "_________________________________________________________________\n",
            "zero3 (ZeroPadding3D)        (None, 77, 8, 14, 64)     0         \n",
            "_________________________________________________________________\n",
            "conv3 (Conv3D)               (None, 75, 6, 12, 96)     165984    \n",
            "_________________________________________________________________\n",
            "batc3 (BatchNormalization)   (None, 75, 6, 12, 96)     384       \n",
            "_________________________________________________________________\n",
            "actv3 (Activation)           (None, 75, 6, 12, 96)     0         \n",
            "_________________________________________________________________\n",
            "spatial_dropout3d_2 (Spatial (None, 75, 6, 12, 96)     0         \n",
            "_________________________________________________________________\n",
            "max3 (MaxPooling3D)          (None, 75, 3, 6, 96)      0         \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 75, 1728)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 75, 512)           3050496   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 75, 512)           1182720   \n",
            "_________________________________________________________________\n",
            "dense1 (Dense)               (None, 75, 28)            14364     \n",
            "_________________________________________________________________\n",
            "softmax (Activation)         (None, 75, 28)            0         \n",
            "=================================================================\n",
            "Total params: 4,575,228\n",
            "Trainable params: 4,574,844\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XItNbaXN0qX"
      },
      "source": [
        "# import tensorflow as tf\n",
        "# tf.test.is_gpu_available(\n",
        "#     cuda_only=False, min_cuda_compute_capability=None\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tHVjJQhN0qa"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMTTdrOBN0qa",
        "outputId": "3dcbe320-6b9a-4a97-d2fa-d330f5539c3c"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "\n",
        "lipnet.model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=adam)\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_gen = data_generator(train_video_path,train_align_path,batch_size = batch_size)\n",
        "val_gen = data_generator(val_video_path,val_align_path, batch_size=batch_size)\n",
        "\n",
        "weight_path = './weights/lipnet_ep{epoch:03d}-{val_loss:.4f}.hdf5'\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(filepath= weight_path,\n",
        "                                  monitor='val_loss',\n",
        "                                  save_best_only=1,\n",
        "                                  mode='auto')\n",
        "\n",
        "history = lipnet.model.fit_generator(generator =train_gen,\n",
        "                              epochs = 100,\n",
        "                              steps_per_epoch = len(train_video_path)/batch_size-1,\n",
        "                              validation_data = val_gen,\n",
        "                              validation_steps = len(val_video_path)/batch_size-1,\n",
        "                              callbacks=checkpoint_callback\n",
        "                              )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            " 13/201 [>.............................] - ETA: 14:04 - loss: 71.4816"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS5hoYIeN0qd"
      },
      "source": [
        "print(train_gen.data_generator())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bstPkEPvN0qf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Iwj5A-lN0qi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}